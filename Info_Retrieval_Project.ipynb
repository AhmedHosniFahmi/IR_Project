{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nznccF8ECEZP"
      },
      "source": [
        "# Information Retrieval and Text Analytics Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bE14IOfCPKo"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i44Dv5q7QOMA"
      },
      "source": [
        "**Overview of Information Retrieval (IR) Systems:** An information retrieval system is a system or software designed to search for and retrieve relevant information from a large collection of unstructured or semi-structured data, such as text documents, images, or multimedia content, based on user queries. These systems are essential in handling vast amounts of information, such as those found on the internet, in digital libraries, or within enterprise data systems.\n",
        "\n",
        "Examples of information retrieval systems:\n",
        "1.   **Google:** A search engine that retrieves relevant web pages based on keywords entered by the user.\n",
        "2.   **Amazon:** An e-commerce platform where users search for products by entering keywords or browsing categories.\n",
        "3.   **PubMed:** A database for medical research articles, allowing users to search for academic papers related to health and science.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZK9X2N4CRJV"
      },
      "source": [
        "**Background:** Data can be found everywhere around us. With the proliferation of digital devices, vast amounts of data are being generated and shared. We can find valuable information in news, books, papers, documentations and wikis. Though to find this valuable information, one must swim through a sea of redundant and sometimes useless data. Therefore, there is a growing need for a way to quickly obtain and sift through this data to extract valuable insights. Many information retrieval models were developed to handle said issue, albeit with varying performances. This creates the issue where we want to know which model performs better in the realm of information retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iP5Yx0NNd9_"
      },
      "source": [
        "\n",
        "**Objective:** This project aims to develop a system that retrieves relevant information from text datasets using 3 different information retrieval models and comparing between their performances to know which one is better. The system also allows us to improve the understanding and organization of the data through text analytics. This includes leveraging preprocessing techniques to enhance text representation, applying robust retrieval methods to ensure accuracy, and utilizing visualizations to provide actionable insights and evaluate performance effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPyQaRx4NesK"
      },
      "source": [
        "**Scope:** The scope includes implementing advanced preprocessing techniques such as tokenization, case standardization, stopwords removal, stemming, and TF-IDF to improve text representation. Additionally, it encompasses the integration of Vector Space Model, Boolean Retrieval Model, and BM25 retrieval algorithms for ranking and identifying relevant information based on user queries. We compare between the performances of these algorithms. To enhance usability, the project will also incorporate visualization tools such as word clouds for top keywords in documents, Frequency distribution of words, document-query similarity scores (e.g., bar charts), and clustering topics using LDA (Latent Dirichlet Allocation). The project will be limited to textual data and will not cover multimedia or non-textual information retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lfdgSg8CjEp"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LITYCPnBisa",
        "outputId": "53a5f945-7e90-42c8-fb10-5368bec2cf7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in ./.venv/lib/python3.12/site-packages (0.3.6)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2024.12.14)\n",
            "Requirement already satisfied: rank_bm25 in ./.venv/lib/python3.12/site-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from rank_bm25) (2.2.1)\n",
            "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/k1ng0a21r/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/k1ng0a21r/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/k1ng0a21r/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/k1ng0a21r/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Downloading required dataset(s)\n",
        "!pip install kagglehub\n",
        "!pip install rank_bm25\n",
        "from rank_bm25 import BM25Okapi\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "import kagglehub\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# path = kagglehub.dataset_download(\"crawford/20-newsgroups\")\n",
        "# The dataset will be download in /home/<user>/.cache/kagglehub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCMSqqcwWrLS"
      },
      "source": [
        "## Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmUr38rBWsxt",
        "outputId": "27879075-479a-49a6-d984-7db334237efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: mathew <mathew@mantis.co.uk>\n",
            "Subject: Alt.Atheism FAQ: Atheist Resources\n",
            "\n",
            "Archive-name: atheis\n"
          ]
        }
      ],
      "source": [
        "# Specify the file path\n",
        "file_path = \"./20-newsgroups/alt.atheism.txt\"  # Replace with the actual path to your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # Open the file and read its contents\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:  # Use 'latin-1' encoding\n",
        "        file_contents = file.read()\n",
        "\n",
        "    # Print or process the file contents\n",
        "    print(file_contents[:100])  # Print the first 1000 characters of the file contents\n",
        "else:\n",
        "    print(f\"Error: File not found at '{file_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing tokenization, lowercasing, stopwords removal, stemming and lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5Qd_kX1swy",
        "outputId": "dfd9a366-6e93-4aa4-b541-a1a2f1796052"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Stopword Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing Vectorization (BoW/TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vectorize_texts(texts, method='bow'):\n",
        "    \"\"\"\n",
        "    Vectorize texts using Bag of Words (BOW) or TF-IDF.\n",
        "\n",
        "    Parameters:\n",
        "    texts (list of str): List of texts to vectorize.\n",
        "    method (str): Method of vectorization ('bow' or 'tfidf').\n",
        "\n",
        "    Returns:\n",
        "    X (sparse matrix): Vectorized text data.\n",
        "    vectorizer (Vectorizer object): Fitted vectorizer.\n",
        "    \"\"\"\n",
        "    if method == 'bow':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'bow' or 'tfidf'\")\n",
        "\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            text = file.read()\n",
        "        return text\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Error: File not found at '{file_path}'\")\n",
        "    \n",
        "text = read_file('./20-newsgroups/alt.atheism.txt')\n",
        "tokens = preprocess_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BoW Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 10031 stored elements and shape (1, 10031)>\n",
            "  Coords\tValues\n",
            "  (0, 6020)\t528\n",
            "  (0, 8734)\t2805\n",
            "  (0, 4027)\t303\n",
            "  (0, 1773)\t2022\n",
            "  (0, 7802)\t57\n",
            "  (0, 12)\t30\n",
            "  (0, 3169)\t9\n",
            "  (0, 133)\t24\n",
            "  (0, 9618)\t207\n",
            "  (0, 1347)\t105\n",
            "  (0, 6740)\t162\n",
            "  (0, 9529)\t96\n",
            "  (0, 4284)\t225\n",
            "  (0, 7731)\t1389\n",
            "  (0, 4253)\t51\n",
            "  (0, 3128)\t45\n",
            "  (0, 4132)\t75\n",
            "  (0, 2292)\t6\n",
            "  (0, 8648)\t9\n",
            "  (0, 1753)\t21\n",
            "  (0, 6882)\t6\n",
            "  (0, 1824)\t93\n",
            "  (0, 9941)\t3015\n",
            "  (0, 4091)\t6\n",
            "  (0, 2192)\t99\n",
            "  :\t:\n",
            "  (0, 1135)\t2\n",
            "  (0, 1136)\t2\n",
            "  (0, 1137)\t2\n",
            "  (0, 1138)\t2\n",
            "  (0, 1139)\t2\n",
            "  (0, 1140)\t2\n",
            "  (0, 1141)\t2\n",
            "  (0, 1142)\t2\n",
            "  (0, 1143)\t2\n",
            "  (0, 1144)\t2\n",
            "  (0, 1145)\t2\n",
            "  (0, 1146)\t2\n",
            "  (0, 1147)\t2\n",
            "  (0, 1148)\t2\n",
            "  (0, 1149)\t2\n",
            "  (0, 1150)\t2\n",
            "  (0, 1151)\t2\n",
            "  (0, 1152)\t2\n",
            "  (0, 1153)\t2\n",
            "  (0, 1154)\t2\n",
            "  (0, 1155)\t2\n",
            "  (0, 1156)\t2\n",
            "  (0, 1157)\t2\n",
            "  (0, 1158)\t2\n",
            "  (0, 1159)\t2\n"
          ]
        }
      ],
      "source": [
        "X_bow, vectorizer_bow = vectorize_texts([tokens], method='bow')\n",
        "print(X_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 10031 stored elements and shape (1, 10031)>\n",
            "  Coords\tValues\n",
            "  (0, 6020)\t0.03652461797590051\n",
            "  (0, 8734)\t0.19403703299697145\n",
            "  (0, 4027)\t0.020960150088442906\n",
            "  (0, 1773)\t0.139872684748619\n",
            "  (0, 7802)\t0.003942998531489259\n",
            "  (0, 12)\t0.0020752623849943468\n",
            "  (0, 3169)\t0.0006225787154983041\n",
            "  (0, 133)\t0.0016602099079954776\n",
            "  (0, 9618)\t0.014319310456460994\n",
            "  (0, 1347)\t0.0072634183474802145\n",
            "  (0, 6740)\t0.011206416878969474\n",
            "  (0, 9529)\t0.00664083963198191\n",
            "  (0, 4284)\t0.015564467887457602\n",
            "  (0, 7731)\t0.09608464842523827\n",
            "  (0, 4253)\t0.0035279460544903898\n",
            "  (0, 3128)\t0.0031128935774915206\n",
            "  (0, 4132)\t0.005188155962485868\n",
            "  (0, 2292)\t0.0004150524769988694\n",
            "  (0, 8648)\t0.0006225787154983041\n",
            "  (0, 1753)\t0.0014526836694960428\n",
            "  (0, 6882)\t0.0004150524769988694\n",
            "  (0, 1824)\t0.006433313393482475\n",
            "  (0, 9941)\t0.20856386969193186\n",
            "  (0, 4091)\t0.0004150524769988694\n",
            "  (0, 2192)\t0.006848365870481345\n",
            "  :\t:\n",
            "  (0, 1135)\t0.0001383508256662898\n",
            "  (0, 1136)\t0.0001383508256662898\n",
            "  (0, 1137)\t0.0001383508256662898\n",
            "  (0, 1138)\t0.0001383508256662898\n",
            "  (0, 1139)\t0.0001383508256662898\n",
            "  (0, 1140)\t0.0001383508256662898\n",
            "  (0, 1141)\t0.0001383508256662898\n",
            "  (0, 1142)\t0.0001383508256662898\n",
            "  (0, 1143)\t0.0001383508256662898\n",
            "  (0, 1144)\t0.0001383508256662898\n",
            "  (0, 1145)\t0.0001383508256662898\n",
            "  (0, 1146)\t0.0001383508256662898\n",
            "  (0, 1147)\t0.0001383508256662898\n",
            "  (0, 1148)\t0.0001383508256662898\n",
            "  (0, 1149)\t0.0001383508256662898\n",
            "  (0, 1150)\t0.0001383508256662898\n",
            "  (0, 1151)\t0.0001383508256662898\n",
            "  (0, 1152)\t0.0001383508256662898\n",
            "  (0, 1153)\t0.0001383508256662898\n",
            "  (0, 1154)\t0.0001383508256662898\n",
            "  (0, 1155)\t0.0001383508256662898\n",
            "  (0, 1156)\t0.0001383508256662898\n",
            "  (0, 1157)\t0.0001383508256662898\n",
            "  (0, 1158)\t0.0001383508256662898\n",
            "  (0, 1159)\t0.0001383508256662898\n"
          ]
        }
      ],
      "source": [
        "X_tfidf, vectorizer_tfidf = vectorize_texts([tokens], method='tfidf')\n",
        "print(X_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read and Process all the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_and_preprocess_files(directory_path='./20-newsgroups'):\n",
        "    preprocessed_texts = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                text = file.read()\n",
        "                preprocessed_text = preprocess_text(text)\n",
        "                preprocessed_texts.append(preprocessed_text)\n",
        "                filenames.append(filename)\n",
        "    return preprocessed_texts, filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'newsgroup 70337 kedz john kedziora subject motorcy'"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed_texts, filenames = read_and_preprocess_files()\n",
        "preprocessed_texts[:][0][:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Set the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"god who created the universe\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing BM25 (Best Matching 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_bm25(query=query, preprocessed_texts=preprocessed_texts, filenames=filenames):\n",
        "    \"\"\"\n",
        "    Apply BM25 algorithm to retrieve and rank documents based on a query.\n",
        "\n",
        "    Parameters:\n",
        "    query (str): The search query.\n",
        "    preprocessed_texts (list of str): List of preprocessed texts.\n",
        "    filenames (list of str): List of filenames corresponding to the texts.\n",
        "\n",
        "    Returns:\n",
        "    list: List of retrieved document filenames.\n",
        "    dict: Dictionary of filenames and their BM25 scores.\n",
        "    \"\"\"\n",
        "    # Tokenize the preprocessed texts\n",
        "    tokenized_texts = [text.split() for text in preprocessed_texts]\n",
        "\n",
        "    # Initialize BM25 with the tokenized corpus\n",
        "    bm25 = BM25Okapi(tokenized_texts)\n",
        "\n",
        "    # Preprocess and tokenize the query\n",
        "    tokenized_query = preprocess_text(query).split()\n",
        "\n",
        "    # Get BM25 scores for the query\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Rank documents based on the scores\n",
        "    top_n = 5\n",
        "    top_n_indices = scores.argsort()[-top_n:][::-1]\n",
        "\n",
        "    # Retrieve the filenames of the top-ranked documents\n",
        "    retrieved_docs = [filenames[i] for i in top_n_indices]\n",
        "\n",
        "    # Create a dictionary of filenames and their scores\n",
        "    ranked_documents = {filenames[i]: scores[i] for i in top_n_indices}\n",
        "\n",
        "    return retrieved_docs, ranked_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retrieved_docs, bm25_ranked_documents = apply_bm25(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alt.atheism.txt': np.float64(4.0251973789462125),\n",
              " 'talk.religion.misc.txt': np.float64(4.024080265336261),\n",
              " 'soc.religion.christian.txt': np.float64(4.01984885336562),\n",
              " 'talk.politics.mideast.txt': np.float64(4.003892292409839),\n",
              " 'talk.politics.misc.txt': np.float64(4.003560019654076)}"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_ranked_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing Vector Space Model (VSM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_vsm(query=query, preprocessed_texts=preprocessed_texts, vectorizer=vectorizer_bow):\n",
        "    # Vectorize the preprocessed texts\n",
        "    X = vectorizer.fit_transform(preprocessed_texts)\n",
        "\n",
        "    # Vectorize the query\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # Calculate cosine similarity between the query and the documents\n",
        "    similarities = cosine_similarity(query_vector, X).flatten()\n",
        "\n",
        "    # Rank documents based on similarity scores\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_indices = ranked_indices[:5]\n",
        "\n",
        "    # Print the ranked documents with filenames and similarity scores\n",
        "    dict = {}\n",
        "    for i in ranked_indices[:]:  # Display top 20 results\n",
        "        dict[filenames[i]] = similarities[i]\n",
        "        # print(f\"Document: {filenames[i]}, Similarity: {similarities[i]}\")\n",
        "\n",
        "    retrieved_documents = [filenames[i] for i in ranked_indices]\n",
        "    return retrieved_documents, dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_bow_retrieved_documents, vsm_bow_ranked_documents = apply_vsm(vectorizer=vectorizer_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'soc.religion.christian.txt': np.float64(0.2563118508119431),\n",
              " 'talk.religion.misc.txt': np.float64(0.19650725528315457),\n",
              " 'alt.atheism.txt': np.float64(0.16728725998947921),\n",
              " 'talk.politics.mideast.txt': np.float64(0.011509771787109557),\n",
              " 'rec.motorcycles.txt': np.float64(0.009676559291175625)}"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vsm_bow_ranked_documents "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_tfidf_retrieved_documents, vsm_tfidf_ranked_documents = apply_vsm(vectorizer=vectorizer_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'soc.religion.christian.txt': np.float64(0.11632702756982942),\n",
              " 'talk.religion.misc.txt': np.float64(0.08837955958448962),\n",
              " 'alt.atheism.txt': np.float64(0.07484612332476384),\n",
              " 'rec.motorcycles.txt': np.float64(0.005874924039153561),\n",
              " 'talk.politics.mideast.txt': np.float64(0.003823562618387289)}"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vsm_tfidf_ranked_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decide the relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: misc.forsale.txt\n",
            "2: alt.atheism.txt\n",
            "3: rec.motorcycles.txt\n",
            "4: comp.windows.x.txt\n",
            "5: soc.religion.christian.txt\n",
            "6: comp.graphics.txt\n",
            "7: sci.crypt.txt\n",
            "8: talk.politics.guns.txt\n",
            "9: comp.sys.ibm.pc.hardware.txt\n",
            "10: sci.space.txt\n",
            "11: rec.sport.baseball.txt\n",
            "12: talk.religion.misc.txt\n",
            "13: sci.electronics.txt\n",
            "14: comp.sys.mac.hardware.txt\n",
            "15: rec.autos.txt\n",
            "16: comp.os.ms-windows.misc.txt\n",
            "17: sci.med.txt\n",
            "18: talk.politics.misc.txt\n",
            "19: rec.sport.hockey.txt\n",
            "20: talk.politics.mideast.txt\n"
          ]
        }
      ],
      "source": [
        "def get_documents_names(directory_path='./20-newsgroups'):\n",
        "    # Create a dictionary of filenames with keys from 1 to 20\n",
        "    filenames_dict = {i+1: filename for i, filename in enumerate(os.listdir(directory_path)[:20])}\n",
        "\n",
        "    # Print the dictionary for user reference\n",
        "    for key, value in filenames_dict.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return filenames_dict\n",
        "\n",
        "filenames_dict = get_documents_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relevant documents: ['talk.politics.guns.txt', 'talk.politics.mideast.txt', 'alt.atheism.txt']\n"
          ]
        }
      ],
      "source": [
        "def get_relevant_documents(filenames_dict):\n",
        "    # Get user input for relevant document numbers\n",
        "    relevant_numbers = input(\"Enter the numbers related to the relevant documents, separated by commas: \")\n",
        "    relevant_numbers = [int(num.strip()) for num in relevant_numbers.split(',')]\n",
        "\n",
        "    # Save the corresponding filenames in a list\n",
        "    relevant_docs = [filenames_dict[num] for num in relevant_numbers if num in filenames_dict]\n",
        "\n",
        "    return relevant_docs\n",
        "\n",
        "# Example usage\n",
        "relevant_docs = get_relevant_documents(filenames_dict)\n",
        "print(f\"Relevant documents: {relevant_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precesion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate precision.\n",
        "    {key: value, key: value}\n",
        "    Parameters:\n",
        "    retrieved_docs (list): List of retrieved document filenames.\n",
        "    relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "    Returns:\n",
        "    float: Precision value.\n",
        "    \"\"\"\n",
        "    # Convert lists to sets for easier calculation\n",
        "    retrieved_set = set(retrieved_docs)\n",
        "    relevant_set = set(relevant_docs)\n",
        "\n",
        "    # Calculate the number of relevant documents retrieved\n",
        "    relevant_retrieved = retrieved_set.intersection(relevant_set)\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = len(relevant_retrieved) / len(retrieved_set) if retrieved_set else 0\n",
        "\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 Precision: 0.4\n",
            "VSM with BoW Precision: 0.4\n",
            "VSM with TF-IDF Precision: 0.4\n"
          ]
        }
      ],
      "source": [
        "# Calculate precision for BM25\n",
        "bm25_precision = calculate_precision(bm25_retrieved_docs, relevant_docs)\n",
        "# Calculate precision for VSM with BoW\n",
        "vsm_bow_precision = calculate_precision(vsm_bow_retrieved_documents, relevant_docs)\n",
        "# Calculate precision for VSM with TF-IDF\n",
        "vsm_tfidf_precision = calculate_precision(vsm_tfidf_retrieved_documents, relevant_docs)\n",
        "\n",
        "print(f\"BM25 Precision: {bm25_precision}\")\n",
        "print(f\"VSM with BoW Precision: {vsm_bow_precision}\")\n",
        "print(f\"VSM with TF-IDF Precision: {vsm_tfidf_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_recall(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate recall.\n",
        "\n",
        "    Parameters:\n",
        "    retrieved_docs (list): List of retrieved document filenames.\n",
        "    relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "    Returns:\n",
        "    float: Recall value.\n",
        "    \"\"\"\n",
        "    # Convert lists to sets for easier calculation\n",
        "    retrieved_set = set(retrieved_docs)\n",
        "    relevant_set = set(relevant_docs)\n",
        "\n",
        "    # Calculate the number of relevant documents retrieved\n",
        "    relevant_retrieved = retrieved_set.intersection(relevant_set)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = len(relevant_retrieved) / len(relevant_set) if relevant_set else 0\n",
        "\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 Recall: 0.6666666666666666\n",
            "VSM with BoW Recall: 0.6666666666666666\n",
            "VSM with TF-IDF Recall: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Calculate recall for BM25\n",
        "bm25_recall = calculate_recall(bm25_retrieved_docs, relevant_docs)\n",
        "# Calculate recall for VSM with BoW\n",
        "vsm_bow_recall = calculate_recall(vsm_bow_retrieved_documents, relevant_docs)\n",
        "# Calculate recall for VSM with TF-IDF\n",
        "vsm_tfidf_recall = calculate_recall(vsm_tfidf_retrieved_documents, relevant_docs)\n",
        "\n",
        "print(f\"BM25 Recall: {bm25_recall}\")\n",
        "print(f\"VSM with BoW Recall: {vsm_bow_recall}\")\n",
        "print(f\"VSM with TF-IDF Recall: {vsm_tfidf_recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Average Precision (MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_average_precision(retrieved_docs_list, relevant_docs_list):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision (MAP).\n",
        "\n",
        "    Parameters:\n",
        "    retrieved_docs_list (list of lists): List of retrieved document filenames for each query.\n",
        "    relevant_docs_list (list of lists): List of relevant document filenames for each query.\n",
        "\n",
        "    Returns:\n",
        "    float: Mean Average Precision value.\n",
        "    \"\"\"\n",
        "    def calculate_average_precision(retrieved_docs, relevant_docs):\n",
        "        \"\"\"\n",
        "        Calculate the average precision for a single query.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        retrieved_docs (list): List of retrieved document filenames.\n",
        "        relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "        Returns:\n",
        "        float: Average precision value.\n",
        "        \"\"\"\n",
        "        retrieved_set = set(retrieved_docs)\n",
        "        relevant_set = set(relevant_docs)\n",
        "        \n",
        "        if not relevant_set:\n",
        "            return 0.0\n",
        "\n",
        "        relevant_retrieved = 0\n",
        "        precision_sum = 0.0\n",
        "\n",
        "        for i, doc in enumerate(retrieved_set):\n",
        "            if doc in relevant_set:\n",
        "                relevant_retrieved += 1\n",
        "                precision_sum += relevant_retrieved / (i + 1)\n",
        "\n",
        "        return precision_sum / len(relevant_set)\n",
        "\n",
        "    average_precisions = [\n",
        "        calculate_average_precision(retrieved_docs, relevant_docs)\n",
        "        for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list)\n",
        "    ]\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions) if average_precisions else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Average Precision: 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Calculate Mean Average Precision\n",
        "retrieved_docs_list = [bm25_retrieved_docs, vsm_bow_retrieved_documents, vsm_tfidf_retrieved_documents]\n",
        "relevant_docs_list = [relevant_docs] * 3\n",
        "mean_average_precision = calculate_mean_average_precision(retrieved_docs_list, relevant_docs_list)\n",
        "print(f\"Mean Average Precision: {mean_average_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'talk.politics.mideast.txt': 430, 'talk.politics.guns.txt': 114, 'talk.politics.misc.txt': 38, 'sci.space.txt': 36, 'comp.graphics.txt': 35, 'soc.religion.christian.txt': 32, 'talk.religion.misc.txt': 20, 'sci.med.txt': 14, 'misc.forsale.txt': 12, 'sci.crypt.txt': 8, 'rec.autos.txt': 8, 'rec.sport.hockey.txt': 8, 'sci.electronics.txt': 6, 'alt.atheism.txt': 3, 'rec.motorcycles.txt': 2, 'comp.sys.ibm.pc.hardware.txt': 2, 'comp.windows.x.txt': 0, 'rec.sport.baseball.txt': 0, 'comp.sys.mac.hardware.txt': 0, 'comp.os.ms-windows.misc.txt': 0}\n"
          ]
        }
      ],
      "source": [
        "def count_word_in_files(directory_path='./20-newsgroups', word=\"army\"):\n",
        "    \"\"\"\n",
        "    Count the occurrences of a word in all files within a directory.\n",
        "\n",
        "    Parameters:\n",
        "    directory_path (str): Path to the directory containing text files.\n",
        "    word (str): Word to count occurrences of.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with filenames as keys and word counts as values, sorted by word counts.\n",
        "    \"\"\"\n",
        "    word_count = {}\n",
        "    word = word.lower()  # Convert the word to lowercase for case-insensitive matching\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                text = file.read()\n",
        "\n",
        "                # Preprocess the text: remove punctuation, convert to lowercase\n",
        "                text = re.sub(r'[^\\w\\s]', '', text)\n",
        "                text = text.lower()\n",
        "\n",
        "                # Count occurrences of the word\n",
        "                count = text.split().count(word)\n",
        "                word_count[filename] = count\n",
        "\n",
        "    # Sort the dictionary by word counts in descending order\n",
        "    sorted_word_count = dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    return sorted_word_count\n",
        "\n",
        "word_counts = count_word_in_files()\n",
        "print(word_counts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
