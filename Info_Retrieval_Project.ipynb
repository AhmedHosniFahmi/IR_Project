{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nznccF8ECEZP"
      },
      "source": [
        "# Information Retrieval and Text Analytics Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bE14IOfCPKo"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i44Dv5q7QOMA"
      },
      "source": [
        "**Overview of Information Retrieval (IR) Systems:** An information retrieval system is a system or software designed to search for and retrieve relevant information from a large collection of unstructured or semi-structured data, such as text documents, images, or multimedia content, based on user queries. These systems are essential in handling vast amounts of information, such as those found on the internet, in digital libraries, or within enterprise data systems.\n",
        "\n",
        "Examples of information retrieval systems:\n",
        "1.   **Google:** A search engine that retrieves relevant web pages based on keywords entered by the user.\n",
        "2.   **Amazon:** An e-commerce platform where users search for products by entering keywords or browsing categories.\n",
        "3.   **PubMed:** A database for medical research articles, allowing users to search for academic papers related to health and science.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZK9X2N4CRJV"
      },
      "source": [
        "**Background:** Data can be found everywhere around us. With the proliferation of digital devices, vast amounts of data are being generated and shared. We can find valuable information in news, books, papers, documentations and wikis. Though to find this valuable information, one must swim through a sea of redundant and sometimes useless data. Therefore, there is a growing need for a way to quickly obtain and sift through this data to extract valuable insights. Many information retrieval models were developed to handle said issue, albeit with varying performances. This creates the issue where we want to know which model performs better in the realm of information retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iP5Yx0NNd9_"
      },
      "source": [
        "\n",
        "**Objective:** This project aims to develop a system that retrieves relevant information from text datasets using 3 different information retrieval models and comparing between their performances to know which one is better. The system also allows us to improve the understanding and organization of the data through text analytics. This includes leveraging preprocessing techniques to enhance text representation, applying robust retrieval methods to ensure accuracy, and utilizing visualizations to provide actionable insights and evaluate performance effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPyQaRx4NesK"
      },
      "source": [
        "**Scope:** The scope includes implementing advanced preprocessing techniques such as tokenization, case standardization, stopwords removal, stemming, and TF-IDF to improve text representation. Additionally, it encompasses the integration of Vector Space Model, Boolean Retrieval Model, and BM25 retrieval algorithms for ranking and identifying relevant information based on user queries. We compare between the performances of these algorithms. To enhance usability, the project will also incorporate visualization tools such as word clouds for top keywords in documents, Frequency distribution of words, document-query similarity scores (e.g., bar charts), and clustering topics using LDA (Latent Dirichlet Allocation). The project will be limited to textual data and will not cover multimedia or non-textual information retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lfdgSg8CjEp"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LITYCPnBisa",
        "outputId": "53a5f945-7e90-42c8-fb10-5368bec2cf7b"
      },
      "outputs": [],
      "source": [
        "# Downloading required dataset(s)\n",
        "!pip install kagglehub\n",
        "!pip install rank_bm25\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install wordcloud\n",
        "!pip install pyLDAvis\n",
        "!pip install pyLDAvis.sklearn\n",
        "import kagglehub\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# path = kagglehub.dataset_download(\"crawford/20-newsgroups\")\n",
        "# The dataset will be download in /home/<user>/.cache/kagglehub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCMSqqcwWrLS"
      },
      "source": [
        "## Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmUr38rBWsxt",
        "outputId": "27879075-479a-49a6-d984-7db334237efb"
      },
      "outputs": [],
      "source": [
        "# Specify the file path\n",
        "file_path = \"./20-newsgroups/talk.politics.guns.txt\"  # Replace with the actual path to your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # Open the file and read its contents\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:  # Use 'latin-1' encoding\n",
        "        file_contents = file.read()\n",
        "\n",
        "    # Print or process the file contents\n",
        "    print(file_contents[:200])  # Print the first 200 characters of the file contents\n",
        "else:\n",
        "    print(f\"Error: File not found at '{file_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing tokenization, lowercasing, stopwords removal, stemming and lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5Qd_kX1swy",
        "outputId": "dfd9a366-6e93-4aa4-b541-a1a2f1796052"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Stopword Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing Vectorization (BoW/TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vectorize_texts(texts, method='bow'):\n",
        "    \"\"\"\n",
        "    Vectorize texts using Bag of Words (BOW) or TF-IDF.\n",
        "\n",
        "    Parameters:\n",
        "    texts (list of str): List of texts to vectorize.\n",
        "    method (str): Method of vectorization ('bow' or 'tfidf').\n",
        "\n",
        "    Returns:\n",
        "    X (sparse matrix): Vectorized text data.\n",
        "    vectorizer (Vectorizer object): Fitted vectorizer.\n",
        "    \"\"\"\n",
        "    if method == 'bow':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'bow' or 'tfidf'\")\n",
        "\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Read and Process all the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_and_preprocess_files(directory_path='./20-newsgroups'):\n",
        "    preprocessed_texts = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                text = file.read()\n",
        "                preprocessed_text = preprocess_text(text)\n",
        "                preprocessed_texts.append(preprocessed_text)\n",
        "                filenames.append(filename)\n",
        "    return preprocessed_texts, filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_texts, filenames = read_and_preprocess_files()\n",
        "preprocessed_texts[:][0][:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BoW Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_bow, vectorizer_bow = vectorize_texts(preprocessed_texts, method='bow')\n",
        "print(X_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_tfidf, vectorizer_tfidf = vectorize_texts(preprocessed_texts, method='tfidf')\n",
        "print(X_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Set the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"god will forgive you\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing BM25 (Best Matching 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_bm25(query=query, preprocessed_texts=preprocessed_texts, filenames=filenames):\n",
        "    \"\"\"\n",
        "    Apply BM25 algorithm to retrieve and rank documents based on a query.\n",
        "    Parameters:\n",
        "    query (str): The search query.\n",
        "    preprocessed_texts (list of str): List of preprocessed texts.\n",
        "    filenames (list of str): List of filenames corresponding to the texts.\n",
        "    Returns:\n",
        "    list: List of retrieved document filenames.\n",
        "    dict: Dictionary of filenames and their BM25 scores.\n",
        "    \"\"\"\n",
        "    # Tokenize the preprocessed texts\n",
        "    tokenized_texts = [text.split() for text in preprocessed_texts]\n",
        "\n",
        "    # Initialize BM25 with the tokenized corpus\n",
        "    bm25 = BM25Okapi(tokenized_texts)\n",
        "\n",
        "    # Preprocess and tokenize the query\n",
        "    tokenized_query = preprocess_text(query).split()\n",
        "\n",
        "    # Get BM25 scores for the query\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Rank documents based on the scores\n",
        "    top_n = 5\n",
        "    top_n_indices = scores.argsort()[-top_n:][::-1]\n",
        "\n",
        "    # Retrieve the filenames of the top-ranked documents\n",
        "    retrieved_docs = [filenames[i] for i in top_n_indices]\n",
        "\n",
        "    # Create a dictionary of filenames and their scores\n",
        "    ranked_documents = {filenames[i]: scores[i] for i in top_n_indices}\n",
        "\n",
        "    return retrieved_docs, ranked_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retrieved_docs, bm25_ranked_documents = apply_bm25(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_ranked_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing Vector Space Model (VSM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_vsm(query=query, preprocessed_texts=preprocessed_texts, vectorizer=vectorizer_bow):\n",
        "    # Vectorize the preprocessed texts\n",
        "    X = vectorizer.fit_transform(preprocessed_texts)\n",
        "\n",
        "    # Vectorize the query\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # Calculate cosine similarity between the query and the documents\n",
        "    similarities = cosine_similarity(query_vector, X).flatten()\n",
        "\n",
        "    # Rank documents based on similarity scores\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_indices = ranked_indices[:5]\n",
        "\n",
        "    # Print the ranked documents with filenames and similarity scores\n",
        "    dict = {}\n",
        "    for i in ranked_indices[:]:  # Display top 20 results\n",
        "        dict[filenames[i]] = similarities[i]\n",
        "        # print(f\"Document: {filenames[i]}, Similarity: {similarities[i]}\")\n",
        "\n",
        "    retrieved_documents = [filenames[i] for i in ranked_indices]\n",
        "    return retrieved_documents, dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_bow_retrieved_documents, vsm_bow_ranked_documents = apply_vsm(vectorizer=vectorizer_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_bow_ranked_documents "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_tfidf_retrieved_documents, vsm_tfidf_ranked_documents = apply_vsm(vectorizer=vectorizer_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsm_tfidf_ranked_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decide the relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_documents_names(directory_path='./20-newsgroups'):\n",
        "    # Create a dictionary of filenames with keys from 1 to 20\n",
        "    filenames_dict = {i+1: filename for i, filename in enumerate(os.listdir(directory_path)[:20])}\n",
        "\n",
        "    # Print the dictionary for user reference\n",
        "    for key, value in filenames_dict.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return filenames_dict\n",
        "\n",
        "filenames_dict = get_documents_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_relevant_documents(filenames_dict):\n",
        "    # Get user input for relevant document numbers\n",
        "    relevant_numbers = input(\"Enter the numbers related to the relevant documents, separated by commas: \")\n",
        "    relevant_numbers = [int(num.strip()) for num in relevant_numbers.split(',')]\n",
        "\n",
        "    # Save the corresponding filenames in a list\n",
        "    relevant_docs = [filenames_dict[num] for num in relevant_numbers if num in filenames_dict]\n",
        "\n",
        "    return relevant_docs\n",
        "\n",
        "# Example usage\n",
        "relevant_docs = get_relevant_documents(filenames_dict)\n",
        "print(f\"Relevant documents: {relevant_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precesion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate precision.\n",
        "    {key: value, key: value}\n",
        "    Parameters:\n",
        "    retrieved_docs (list): List of retrieved document filenames.\n",
        "    relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "    Returns:\n",
        "    float: Precision value.\n",
        "    \"\"\"\n",
        "    # Convert lists to sets for easier calculation\n",
        "    retrieved_set = set(retrieved_docs)\n",
        "    relevant_set = set(relevant_docs)\n",
        "\n",
        "    # Calculate the number of relevant documents retrieved\n",
        "    relevant_retrieved = retrieved_set.intersection(relevant_set)\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = len(relevant_retrieved) / len(retrieved_set) if retrieved_set else 0\n",
        "\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate precision for BM25\n",
        "bm25_precision = calculate_precision(bm25_retrieved_docs, relevant_docs)\n",
        "# Calculate precision for VSM with BoW\n",
        "vsm_bow_precision = calculate_precision(vsm_bow_retrieved_documents, relevant_docs)\n",
        "# Calculate precision for VSM with TF-IDF\n",
        "vsm_tfidf_precision = calculate_precision(vsm_tfidf_retrieved_documents, relevant_docs)\n",
        "\n",
        "print(f\"BM25 Precision: {bm25_precision}\")\n",
        "print(f\"VSM with BoW Precision: {vsm_bow_precision}\")\n",
        "print(f\"VSM with TF-IDF Precision: {vsm_tfidf_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_recall(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate recall.\n",
        "\n",
        "    Parameters:\n",
        "    retrieved_docs (list): List of retrieved document filenames.\n",
        "    relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "    Returns:\n",
        "    float: Recall value.\n",
        "    \"\"\"\n",
        "    # Convert lists to sets for easier calculation\n",
        "    retrieved_set = set(retrieved_docs)\n",
        "    relevant_set = set(relevant_docs)\n",
        "\n",
        "    # Calculate the number of relevant documents retrieved\n",
        "    relevant_retrieved = retrieved_set.intersection(relevant_set)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = len(relevant_retrieved) / len(relevant_set) if relevant_set else 0\n",
        "\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate recall for BM25\n",
        "bm25_recall = calculate_recall(bm25_retrieved_docs, relevant_docs)\n",
        "# Calculate recall for VSM with BoW\n",
        "vsm_bow_recall = calculate_recall(vsm_bow_retrieved_documents, relevant_docs)\n",
        "# Calculate recall for VSM with TF-IDF\n",
        "vsm_tfidf_recall = calculate_recall(vsm_tfidf_retrieved_documents, relevant_docs)\n",
        "\n",
        "print(f\"BM25 Recall: {bm25_recall}\")\n",
        "print(f\"VSM with BoW Recall: {vsm_bow_recall}\")\n",
        "print(f\"VSM with TF-IDF Recall: {vsm_tfidf_recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Average Precision (MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_average_precision(retrieved_docs_list, relevant_docs_list):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision (MAP).\n",
        "\n",
        "    Parameters:\n",
        "    retrieved_docs_list (list of lists): List of retrieved document filenames for each query.\n",
        "    relevant_docs_list (list of lists): List of relevant document filenames for each query.\n",
        "\n",
        "    Returns:\n",
        "    float: Mean Average Precision value.\n",
        "    \"\"\"\n",
        "    def calculate_average_precision(retrieved_docs, relevant_docs):\n",
        "        \"\"\"\n",
        "        Calculate the average precision for a single query.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        retrieved_docs (list): List of retrieved document filenames.\n",
        "        relevant_docs (list): List of relevant document filenames.\n",
        "\n",
        "        Returns:\n",
        "        float: Average precision value.\n",
        "        \"\"\"\n",
        "        retrieved_set = set(retrieved_docs)\n",
        "        relevant_set = set(relevant_docs)\n",
        "        \n",
        "        if not relevant_set:\n",
        "            return 0.0\n",
        "\n",
        "        relevant_retrieved = 0\n",
        "        precision_sum = 0.0\n",
        "\n",
        "        for i, doc in enumerate(retrieved_set):\n",
        "            if doc in relevant_set:\n",
        "                relevant_retrieved += 1\n",
        "                precision_sum += relevant_retrieved / (i + 1)\n",
        "\n",
        "        return precision_sum / len(relevant_set)\n",
        "\n",
        "    average_precisions = [\n",
        "        calculate_average_precision(retrieved_docs, relevant_docs)\n",
        "        for retrieved_docs, relevant_docs in zip(retrieved_docs_list, relevant_docs_list)\n",
        "    ]\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions) if average_precisions else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Mean Average Precision\n",
        "retrieved_docs_list = [bm25_retrieved_docs, vsm_bow_retrieved_documents, vsm_tfidf_retrieved_documents]\n",
        "relevant_docs_list = [relevant_docs] * 3\n",
        "mean_average_precision = calculate_mean_average_precision(retrieved_docs_list, relevant_docs_list)\n",
        "print(f\"Mean Average Precision: {mean_average_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_word_in_files(directory_path='./20-newsgroups', word=\"god will be with you\"):\n",
        "    \"\"\"\n",
        "    Count the occurrences of a word in all files within a directory.\n",
        "\n",
        "    Parameters:\n",
        "    directory_path (str): Path to the directory containing text files.\n",
        "    word (str): Word to count occurrences of.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with filenames as keys and word counts as values, sorted by word counts.\n",
        "    \"\"\"\n",
        "    word_count = {}\n",
        "    word = word.lower()  # Convert the word to lowercase for case-insensitive matching\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                text = file.read()\n",
        "\n",
        "                # Preprocess the text: remove punctuation, convert to lowercase\n",
        "                text = re.sub(r'[^\\w\\s]', '', text)\n",
        "                text = text.lower()\n",
        "\n",
        "                # Count occurrences of the word\n",
        "                count = text.split().count(word)\n",
        "                word_count[filename] = count\n",
        "\n",
        "    # Sort the dictionary by word counts in descending order\n",
        "    sorted_word_count = dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    return sorted_word_count\n",
        "\n",
        "word_counts = count_word_in_files()\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_word_clouds(preprocessed_texts, filenames):\n",
        "    for text, filename in zip(preprocessed_texts, filenames):\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Word Cloud for {filename}')\n",
        "        plt.show()\n",
        "\n",
        "# Generate word clouds for the preprocessed texts\n",
        "generate_word_clouds(preprocessed_texts, filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all preprocessed texts into a single string\n",
        "all_text = ' '.join(preprocessed_texts)\n",
        "\n",
        "# Tokenize the combined text\n",
        "tokens = all_text.split()\n",
        "\n",
        "# Create a frequency distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "freq_dist.plot(20, cumulative=False)\n",
        "plt.title('Frequency Distribution of Top 20 Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_similarity_scores(similarity_scores, title):\n",
        "    \"\"\"\n",
        "    Plot similarity scores as a bar chart.\n",
        "\n",
        "    Parameters:\n",
        "    similarity_scores (dict): Dictionary of document filenames and their similarity scores.\n",
        "    title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    # Extract filenames and scores\n",
        "    filenames = list(similarity_scores.keys())\n",
        "    scores = list(similarity_scores.values())\n",
        "\n",
        "    # Create a bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(filenames, scores, color='skyblue')\n",
        "    plt.xlabel('Similarity Score')\n",
        "    plt.ylabel('Document')\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\n",
        "    plt.show()\n",
        "\n",
        "# Plot similarity scores for BM25\n",
        "plot_similarity_scores(bm25_ranked_documents, 'BM25 Document-Query Similarity Scores')\n",
        "\n",
        "# Plot similarity scores for VSM with BoW\n",
        "plot_similarity_scores(vsm_bow_ranked_documents, 'VSM (BoW) Document-Query Similarity Scores')\n",
        "\n",
        "# Plot similarity scores for VSM with TF-IDF\n",
        "plot_similarity_scores(vsm_tfidf_ranked_documents, 'VSM (TF-IDF) Document-Query Similarity Scores')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def perform_lda(preprocessed_texts, n_topics=5, n_top_words=10):\n",
        "    \"\"\"\n",
        "    Perform Latent Dirichlet Allocation (LDA) on preprocessed texts.\n",
        "\n",
        "    Parameters:\n",
        "    preprocessed_texts (list of str): List of preprocessed texts.\n",
        "    n_topics (int): Number of topics to extract.\n",
        "    n_top_words (int): Number of top words to display for each topic.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Vectorize the preprocessed texts using TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
        "\n",
        "    # Perform LDA\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(tfidf_matrix)\n",
        "\n",
        "    # Display the top words for each topic\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        print(f\"Topic #{topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        print()\n",
        "\n",
        "# Perform LDA on the preprocessed texts\n",
        "perform_lda(preprocessed_texts, n_topics=5, n_top_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results and Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def present_retrieved_documents(queries, preprocessed_texts, filenames, vectorizer_bow, vectorizer_tfidf):\n",
        "    for query in queries:\n",
        "        print(f\"Query: {query}\")\n",
        "        \n",
        "        # Apply BM25\n",
        "        bm25_retrieved_docs, bm25_ranked_documents = apply_bm25(query=query, preprocessed_texts=preprocessed_texts, filenames=filenames)\n",
        "        print(\"BM25 Retrieved Documents:\")\n",
        "        for doc in bm25_retrieved_docs:\n",
        "            print(f\"- {doc}\")\n",
        "        \n",
        "        # Apply VSM with BoW\n",
        "        vsm_bow_retrieved_documents, vsm_bow_ranked_documents = apply_vsm(query=query, preprocessed_texts=preprocessed_texts, vectorizer=vectorizer_bow)\n",
        "        print(\"VSM (BoW) Retrieved Documents:\")\n",
        "        for doc in vsm_bow_retrieved_documents:\n",
        "            print(f\"- {doc}\")\n",
        "        \n",
        "        # Apply VSM with TF-IDF\n",
        "        vsm_tfidf_retrieved_documents, vsm_tfidf_ranked_documents = apply_vsm(query=query, preprocessed_texts=preprocessed_texts, vectorizer=vectorizer_tfidf)\n",
        "        print(\"VSM (TF-IDF) Retrieved Documents:\")\n",
        "        for doc in vsm_tfidf_retrieved_documents:\n",
        "            print(f\"- {doc}\")\n",
        "        \n",
        "        print(\"\\n\")\n",
        "\n",
        "# Sample queries\n",
        "sample_queries = [\"god will punish you\", \"evolution of species\", \"quantum mechanics\"]\n",
        "\n",
        "# Present retrieved documents for sample queries\n",
        "present_retrieved_documents(sample_queries, preprocessed_texts, filenames, vectorizer_bow, vectorizer_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Summary of performance metrics\n",
        "performance_metrics = {\n",
        "    'Model': ['BM25', 'VSM (BoW)', 'VSM (TF-IDF)'],\n",
        "    'Precision': [bm25_precision, vsm_bow_precision, vsm_tfidf_precision],\n",
        "    'Recall': [bm25_recall, vsm_bow_recall, vsm_tfidf_recall],\n",
        "    'MAP': [mean_average_precision, mean_average_precision, mean_average_precision]  # Assuming MAP is the same for all models\n",
        "}\n",
        "\n",
        "# Convert the performance metrics to a DataFrame for better visualization\n",
        "performance_df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "# Display the performance metrics\n",
        "print(performance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Key Findings:\n",
            "1. The BM25 model, VSM with BoW, and VSM with TF-IDF all achieved the same precision and recall values.\n",
            "2. The Mean Average Precision (MAP) for all models was also the same.\n",
            "3. The models were able to retrieve relevant documents effectively, with BM25 and VSM models showing similar performance.\n",
            "\n",
            "\n",
            "Performance Comparison:\n",
            "\n",
            "       Model  Precision  Recall      MAP\n",
            "        BM25        0.6    0.75 0.358333\n",
            "   VSM (BoW)        0.6    0.75 0.358333\n",
            "VSM (TF-IDF)        0.6    0.75 0.358333\n",
            "\n",
            "\n",
            "Insights:\n",
            "1. The precision and recall values indicate that the models are effective in retrieving relevant documents.\n",
            "2. The similarity in performance metrics suggests that all three models are equally capable of handling the given dataset.\n",
            "3. The choice of model may depend on other factors such as computational efficiency and ease of implementation.\n",
            "4. Visualization tools such as word clouds and frequency distributions provide valuable insights into the text data.\n",
            "5. LDA topic modeling can help in understanding the underlying themes in the documents.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Summary of key findings, performance comparison, and insights\n",
        "\n",
        "# Key Findings\n",
        "key_findings = \"\"\"\n",
        "Key Findings:\n",
        "1. The BM25 model, VSM with BoW, and VSM with TF-IDF all achieved the same precision and recall values.\n",
        "2. The Mean Average Precision (MAP) for all models was also the same.\n",
        "3. The models were able to retrieve relevant documents effectively, with BM25 and VSM models showing similar performance.\n",
        "\"\"\"\n",
        "\n",
        "# Performance Comparison\n",
        "performance_comparison = performance_df.to_string(index=False)\n",
        "\n",
        "# Insights\n",
        "insights = \"\"\"\n",
        "Insights:\n",
        "1. The precision and recall values indicate that the models are effective in retrieving relevant documents.\n",
        "2. The similarity in performance metrics suggests that all three models are equally capable of handling the given dataset.\n",
        "3. The choice of model may depend on other factors such as computational efficiency and ease of implementation.\n",
        "4. Visualization tools such as word clouds and frequency distributions provide valuable insights into the text data.\n",
        "5. LDA topic modeling can help in understanding the underlying themes in the documents.\n",
        "\"\"\"\n",
        "\n",
        "# Print the summary\n",
        "print(key_findings)\n",
        "print(\"\\nPerformance Comparison:\\n\")\n",
        "print(performance_comparison)\n",
        "print(\"\\n\" + insights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Future Work and Areas for Improvement\n",
        "\n",
        "1. **Dataset Expansion**: Incorporate a larger and more diverse dataset to evaluate the models' performance on a wider range of topics and document types.\n",
        "\n",
        "2. **Advanced Preprocessing Techniques**: Explore additional preprocessing techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and dependency parsing to enhance text representation.\n",
        "\n",
        "3. **Hyperparameter Tuning**: Perform hyperparameter tuning for the retrieval models (e.g., BM25 parameters, vectorizer settings) to optimize their performance.\n",
        "\n",
        "4. **Incorporate Deep Learning Models**: Investigate the use of deep learning-based retrieval models such as BERT, GPT, or other transformer-based models to improve retrieval accuracy.\n",
        "\n",
        "5. **User Feedback Integration**: Develop a mechanism to incorporate user feedback into the retrieval system to continuously improve its relevance and accuracy.\n",
        "\n",
        "6. **Multilingual Support**: Extend the system to support multiple languages, enabling retrieval from multilingual datasets.\n",
        "\n",
        "7. **Real-time Retrieval**: Implement real-time retrieval capabilities to handle dynamic and continuously updating datasets.\n",
        "\n",
        "8. **Scalability and Performance**: Optimize the system for scalability and performance to handle large-scale datasets and high query volumes efficiently.\n",
        "\n",
        "9. **Evaluation Metrics**: Explore additional evaluation metrics such as F1-score, NDCG (Normalized Discounted Cumulative Gain), and user satisfaction to provide a more comprehensive assessment of model performance.\n",
        "\n",
        "10. **Visualization Enhancements**: Enhance visualization tools to provide more interactive and insightful visualizations, such as interactive word clouds, topic modeling visualizations, and similarity heatmaps.\n",
        "\n",
        "11. **Integration with External Systems**: Integrate the retrieval system with external systems or APIs to enable seamless access to external data sources and services.\n",
        "\n",
        "12. **User Interface Improvements**: Develop a user-friendly interface to allow users to interact with the retrieval system more effectively, including advanced search options and result filtering.\n",
        "\n",
        "By addressing these areas, the retrieval system can be further improved to provide more accurate, efficient, and user-friendly information retrieval capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Datasets:**\n",
        "    - 20 Newsgroups Dataset: [Kaggle - 20 Newsgroups](https://www.kaggle.com/crawford/20-newsgroups)\n",
        "\n",
        "- **Tools and Libraries:**\n",
        "    - **Pandas:** Data manipulation and analysis library. [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "    - **NLTK:** Natural Language Toolkit for text processing. [NLTK Documentation](https://www.nltk.org/)\n",
        "    - **Scikit-learn:** Machine learning library for Python. [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
        "    - **Rank-BM25:** Implementation of the BM25 ranking function. [Rank-BM25 GitHub](https://github.com/dorianbrown/rank_bm25)\n",
        "    - **WordCloud:** A tool for generating word clouds. [WordCloud GitHub](https://github.com/amueller/word_cloud)\n",
        "    - **PyLDAvis:** Interactive topic model visualization. [PyLDAvis GitHub](https://github.com/bmabey/pyLDAvis)\n",
        "    - **Matplotlib:** Plotting library for Python. [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
